{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ipympl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-fa0befb2b5a5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mipympl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'ipympl'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import ipympl\n",
    "import datetime, time\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from sklearn.metrics import roc_curve, auc #for model evaluation\n",
    "from sklearn.metrics import classification_report #for model evaluation\n",
    "from sklearn.metrics import confusion_matrix #for model evaluation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "import chart_studio.plotly as pltly\n",
    "import plotly.graph_objects as go\n",
    "import plotly.offline as poff\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.figure_factory as ff\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "import psutil\n",
    "import qgrid   \n",
    "from ipywidgets import interact, interact_manual, Layout\n",
    "import ipywidgets as widgets\n",
    "import warnings\n",
    "\n",
    "init_notebook_mode(connected=True)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tooltip\n",
    "\n",
    "Um die Interaktiven Grafiken voll ausnutzen zu können für Sie ein paar Infos. Die Grafiken welche eine Auswahlmöglichkeit bieten, ermöglichen es Ihnen mehrere sogenannte \"Features\" gleichzeitig auszuwählen. Eine Multiselektion können sie mit ```Strg + linksklick```erreichen. <img src=\"s.png\" width=\"200\">\n",
    "\n",
    "Innerhalb der Grafiken können Sie einzelne Elemente genauer ansehen, sie können mit halten der linken Maustaste einfach einen gewünschten Ausschnitt auswählen. Um die Auswahl zurücksetzen zu können, gibt es rechts oben eine Leiste mit Icons. Klicken Sie einfach auf das Icon mit dem Haus, damit wird die Grafik auf Ihren Ursprungszustand zurückgesetzt.\n",
    "\n",
    "<img src=\"icons.png\" width=\"200\">\n",
    "\n",
    "Sollten Sie noch Fragen haben können Sie uns gerne persönlich Fragen!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projektbeschreibung\n",
    "## Entwicklung eines innovativen Geschäftsmodells in der Digitalen Wirtschaft –\n",
    "## Masterprojekt\n",
    "\n",
    "Um die Theorie des Studiums mit der Praxis des Arbeitsalltags zu verknüpfen, soll dieses\n",
    "Projekt dazu dienen, Einblicke und neue Erkenntnisse der digitalen Welt, insbesondere dem\n",
    "Bereich der Industrie 4.0 zu erlangen. Durch die Arbeitsumgebung des Digital Laboratory\n",
    "werden Grundlagen und Hilfsmittel zur Durchführung des Projekts bereitgestellt.\n",
    "Die vorhandene 5-Achs-Fräsmaschine der Firma DMG Mori liefert dafür den Use-Case und\n",
    "die dazu benötigte Datengrundlage für das Projekt.\n",
    "Hierbei gilt es die Welt des Maschinenbaus mit der Welt der Informatik zu verknüpfen. Dies\n",
    "soll durch Nutzung von Machine Learning Technologien geschehen.\n",
    "\n",
    "\n",
    "Um das Projekt fachlich korrekt durchzuführen, müssen sich die Studierenden Wissen aus\n",
    "beiden Bereichen aneignen. Diese Wissensaneignung erfolgt durch individuelle Erkundung\n",
    "einer Thematik und dem anschließenden Austausch des gewonnen Wissens mit dem\n",
    "restlichen Projektteam in Form eines Knowledge-Sheets. Zu diesen Grundlagen gehören\n",
    "neben Technologien wie TensorFlow und Python auch die zum Machine Learning gehörigen\n",
    "Lernmethoden Supervised-/Unsupervised-/Reinforcement-Learning und deren Algorithmen\n",
    "wie Random Forest, k-Nearest-Neighbour, Support Vector Machines, Lineare Regression und\n",
    "viele weitere. Neben den Informatikinhalten muss aber auch Wissen über die\n",
    "Funktionsweise der Maschine angeeignet und vermittelt werden.\n",
    "\n",
    "\n",
    "Sind die Grundlagen vermittelt worden, wird der Versuchsaufbau, welcher die Grundlage der\n",
    "Anwendung von Machine Learning darstellt, beschrieben und anschließend durchgeführt.\n",
    "Hier sollen mit der 5-Achs-Fräsmaschine und mehrere hundert Passungen nacheinander aus\n",
    "einem Stahlblock gefräst werden. Dazu soll die Toleranz jeder Passung mit dem Messmittel\n",
    "der Maschine ermittelt werden, um festzustellen, ab welchem Zeitpunkt bzw. welche\n",
    "Umstände dazu führen, dass die gefrästen Löcher außerhalb des Toleranzbereichs liegen. Die\n",
    "Beurteilung erfolgt später über die Kennzeichnung (Labeling) der Löcher in die Kategorien\n",
    "IO/NIO.\n",
    "\n",
    "\n",
    "Die gewonnen Daten werden in einer Datenbank gespeichert. Hierbei werden ETL-Prozesse\n",
    "herangezogen um die Daten von der Datenbank zum DataLake transferieren zu können. Auf\n",
    "einem Server sollen die Daten dann extrahiert und in ein DataWarehouse gespeichert\n",
    "werden, um diese später auswerten zu können. Nachdem der ETL-Prozess abgeschlossen ist\n",
    "können die Daten verwendet werden, um Vorhersagen mit Machine Learning Algorithmen\n",
    "zu treffen. Um die Rohdaten passend aufzubereiten, gilt es zuerst geeignete Parameter zu\n",
    "finden. Nachdem die Parameter identifiziert wurden und die zugehörigen Daten verarbeitet\n",
    "sind können diese den verschiedenen Machine Learning Algorithmen zur Verfügung gestellt\n",
    "werden.\n",
    "\n",
    "\n",
    "Um einen reibungslosen Ablauf der Trainings für die Machine Learning Prozesse zu\n",
    "gewährleisten soll eine ML-Umgebung aufgebaut werden, damit nicht jeder Algorithmus\n",
    "individuell konfiguriert werden muss. Diese Umgebung soll möglichst stabil sein, sodass es\n",
    "im laufenden Prozess zu möglichst wenigen Störungen kommt.\n",
    "\n",
    "\n",
    "Das Ziel des Machine Learning soll sein eine Identifizierung der Einflussfaktoren zu erhalten,\n",
    "soll heißen welche Parameter einen starken, aktiven Einfluss auf den Fräsprozess haben und\n",
    "somit das Fräsergebnis beeinflussen. Diese Informationen können genutzt werden um später\n",
    "Machine Learning Modelle in Echtzeit für Fräsprozesse einsetzen zu können."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masterprojekt_daten = pd.read_csv(\"./Messergebnisse_merged.csv\")\n",
    "masterprojekt_daten = masterprojekt_daten.drop(columns=['Unnamed: 0'], axis=0)\n",
    "#print(masterprojekt_daten.columns, masterprojekt_daten.dtypes)\n",
    "\n",
    "#print(masterprojekt_daten.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masterprojekt_daten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning\n",
    "\n",
    "Machine Learning ist ein Teilbereich der Künstlichen Intelligenz, der es einem System ermöglicht, mit Hilfe von lernenden Algorithmen zusammenhänge in Daten zu erkennen. Diese zusammenhänge bezeichnet man als Muster, diese haben einen gleichförmige Struktur. Um Muster in Daten erkennen zu können benötigt Daten um in solchen bestimmte Gesetzmäßigkeiten zu erkennen und entsprechende Schlüsse ziehen zu können. \n",
    "\n",
    "<img src=\"mluebersicht.png\" width=\"300\">\n",
    "\n",
    "Damit die Algorithmen erfolgreich Muster erkennen können, wird eine repräsentative Stichprobe des Anwendungsgebiets benötigt. Um eine repräsentative Stichprobe zu erhalten müssen Daten gesammelt werden. Nachdem die Stichprobe ermittelt und validiert wurde, werden Merkmale gewählt. Die Merkmalauswahl ist nicht einfach, da man die Anzahl der Merkmale beachten muss. Da bei mehr als drei Merkmalen die Visualisierung nicht mehr möglich ist, ist das finden von Mustern eine komplexe Aufgabe. Außerdem beinhalten nicht alle Merkmale wichtige Informationen zur Klassifikation, diese können dann entfernt werden. Anhand der Datenstrukturen und Eigenschaften kann ein Modell gewählt werden. Dabei kann man zwischen Klassifikatoren und Regressoren unterscheiden.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple und Logistische Regression\n",
    "\n",
    "## Was ist Regression?\n",
    "\n",
    "Bei linearer Regression sucht man nach Beziehungen zwischen einer Zielgröße $Y$ und den zugehörigen Einflussgrößen, den abhängigen Variablen $X$, wobei die Anzahl der Einflussgrößen $X$ kontinuierlich ist. Das dabei entstehende lineare Modell wird auf seine Zusammenhänge zwischen abhängigen Variablen der Linearkombination geprüft. Wenn man Regression nutzt möchte man meist Wissen wie sich bestimmte Einflussgrößen auf das Verhalten, bzw. auf den Zusammenhang auswirken. Dafür müssen alle Einflussgrößen $X$ auch **tatsächlich** unabhängig von der Zielvariablen sein.\n",
    "Was hierbei schnell hervorgeht, es muss eine **Abhängigkeit** vorliegen. Die dabei gängige Formel lautet: \n",
    "\n",
    "$ Y \\approx \\alpha + \\beta X + \\varepsilon $\n",
    "\n",
    "Es gibt mehrere Notationen, welche alle dieselbe Aussage haben und wie folgt aussehen können:\n",
    "\n",
    "- $a + b X$,\n",
    "- $\\beta_0 + \\beta_1 X$.\n",
    "\n",
    "Die Variablen $\\alpha$ und $\\beta$ geben den Achsenabschnitt und die Steigung an. $\\varepsilon$ gibt den Fehler der vorhersage an. Hier kann die Fehlerfunktion beliebig gewählt werden, am häufigsten ist der MSE (Mean-Squared-Error) zu finden. Die Formel lautet:\n",
    "\n",
    "$MSE = \\frac{\\sum^n_{i=1}{(y_i - y_i^p)}^2}{n} $\n",
    "\n",
    "Wie jedoch an unseren Daten erkennbar, ist für wenige, bis keine Features eine Linearität gegeben. Folglich ist die Anwendung von Linearer und Multipler Linearer Regression wenig sinnvoll. Jedoch kann an dieser Stelle die Logistische Regression angewendet werden.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_notebook_mode(connected=True) \n",
    "classes=np.unique(masterprojekt_daten.columns).tolist()\n",
    "class_code={classes[k]: k for k in range(len(masterprojekt_daten.columns))}\n",
    "color_vals=[class_code[cl] for cl in masterprojekt_daten.columns]\n",
    "\n",
    "selection = widgets.SelectMultiple(\n",
    "    options=masterprojekt_daten.columns,\n",
    "    rows=5,\n",
    "    value=['Energy_Savings.Active_Power_Spindle'],\n",
    "    description='Spalten',\n",
    "    disabled=False,\n",
    "    layout=Layout(width=\"40%\")\n",
    ")\n",
    "\n",
    "@interact(selection=selection)\n",
    "def splotMatrix(selection):\n",
    "    dimension = [dict(label=str(x), values=masterprojekt_daten[x]) for x in selection]\n",
    "    splom_matrix = go.Splom(dimensions=[column for column in dimension], \n",
    "                            marker=dict(color=masterprojekt_daten['Label'], colorscale=\"cividis\"))\n",
    "\n",
    "    fig = go.Figure(dict(data=[splom_matrix]))\n",
    "    fig.update_layout(font=dict(size=8),\n",
    "            yaxis=dict(\n",
    "            autorange=True,\n",
    "            type=\"linear\",\n",
    "            showticklabels=False\n",
    "        ),\n",
    "            xaxis=dict(\n",
    "            autorange=True))\n",
    "    \n",
    "    fig['layout'].update(height=1000, width=1000)\n",
    "\n",
    "    iplot(fig)\n",
    "\n",
    "\n",
    "# poff.plot(fig, filename='splom.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistische Regression\n",
    "\n",
    "In der Logistischen Regression gibt es nicht wie bei der Linearen Regression unendlich viele, sondern endlich viele Möglichkeiten. Das Ziel ist, das Finden von Beziehungen (Abhängigkeiten) zwischen der Zielvariablen (independent) und den abhängigen Variablen (dependent), die Zielvariable muss kategorisch binär sein. Heißt sie darf nur as zwei Klassen bestehen, wie in unserem Beispiel:\n",
    "\n",
    "- Werkzeugzustand (worn | new)\n",
    "- Bewertung durch visuelle Inspektion (i.O. | n.i.O.)\n",
    "- Fertiggestellt (ja | nein)\n",
    "\n",
    "\n",
    "Für diese Klassen wird die Wahrscheinlichkeit ermittelt.\n",
    "Die Logistische Regression ist eine spezielle Form der Linearen Regression, wobei die Zielvariable von kategorischer Natur ist. Bedeutet hier; Lineare Regression gibt einen kontinuierlichen Output, Logistische Regression hingegen einen konstanten Output. Für die Wahrscheinlichkeitsberechnung wird eine Logit-Funktion verwendet, dabei gibt es einige Eigenschaften:\n",
    "\n",
    "- Die Zielvariable entspricht einer Bernoulliverteilung\n",
    "- Die Schätzung erfolgt über Maximum-Likelihood \n",
    "\n",
    "\n",
    "$y = \\beta 0 + \\beta 1 X1 + \\beta 2 X2 + ... + \\beta nXn$\n",
    "\n",
    "Die Zielvariable ist $y$, welche wir minimieren möchten. $\\beta$ beschreibt den Y-Achsenabschnitt, der Punkt an dem die Strecke den Achsenabschnitt schneidet. $\\beta 1$ ist die Steigung der Strecke und $X$ repräsentiert die unabhängigen Variable, die man nutzt um eine voraussage zu treffen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "x, y = make_classification(\n",
    "    n_samples=500,\n",
    "    n_features=1,\n",
    "    n_classes=2,\n",
    "    n_clusters_per_class=1,\n",
    "    flip_y=0.03,\n",
    "    n_informative=1,\n",
    "    n_redundant=0,\n",
    "    n_repeated=0\n",
    ")\n",
    "\n",
    "x_train_example, x_test_example, y_train_example, y_test_example = train_test_split(x, y, random_state=1)\n",
    "\n",
    "lr = LogisticRegression()\n",
    "lr.fit(x_train_example, y_train_example)\n",
    "\n",
    "y_pred = lr.predict(x_test_example)\n",
    "\n",
    "df_example = pd.DataFrame({'x': x_test_example[:,0], 'y': y_test_example})\n",
    "df_example = df_example.sort_values(by='x')\n",
    "from scipy.special import expit\n",
    "sigmoid_function = expit(df_example['x'] * lr.coef_[0][0] + lr.intercept_[0]).ravel()\n",
    "plt.plot(df_example['x'], sigmoid_function)\n",
    "plt.scatter(df_example['x'], df_example['y'], c=df_example['y'], cmap='rainbow', edgecolors='b')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Daten und deren Bedeutung (2. Daten aufbereiten)\n",
    "\n",
    "Wie im Beschreibungstext ersichtlich wird sind die gesammelten Daten nicht Fehlerfrei. Dieser Schluss kommt nicht überraschend. Daten aus der realen Welt sind nie Fehlerfrei. Da Daten selten eine hohe *Reinheit* besitzen oder *Homogen* (gleiche Eigenschaften, Typen) sind, muss viel Zeit darauf verwendet werden diese entweder zu bereinigen oder zu entfernen. Generell gibt es zwei Arten von problematischen Daten:\n",
    "\n",
    "1. Fehlende Daten\n",
    "2. Falsche Daten\n",
    "\n",
    "Fehlende Daten sind ein Problem das bei jeder Datenerhebung entseht. Manchmal ist bekannt weshalb Daten fehlen, manchmal nicht. Der Knackpunkt ist jedoch der Umgang mit den fehlenden Daten (manchmal kann es jedoch auch helfen wenn man weiß wieso Daten fehlen, dies kann dann bei der nächsten Erhebung mit einbezogen werden). Das Problem an fehlenden Daten ist der verfälschte, bzw. nicht vorhandene Informationswert. Fehlende Daten können von ganzen Messreihen, bis zu einzelnen Messergebnissen etc. reichen. Der Umgang mit solchen soll hier kurz Umrissen werden:\n",
    "\n",
    "- Allgemeine Ansätze:\n",
    "    - Löschen der fehlenden Daten, dieser Ansatz sollte aber nur bei einer sehr kleinen Menge von fehlenden Daten oder bei einem insignifikanten Datensatz verwendet werden\n",
    "    - Ersetzen der fehlenden Daten durch vorherigen/folgenden Wert, auch hier sollte der Ansatz nur bei einer kleinen Menge von Daten gewählt werden\n",
    "    - Ersetzen der fehlenden Daten durch eine globale Konstante, kann hilfreich sein um fehlende Daten besser Filtern und extrahieren zu können\n",
    "\n",
    "\n",
    "- Behandlung von fehlenden Numerischen Werten\n",
    "    - Ersetzen der Daten durch den Mittelwert. Der Mittelwert bietet sich für fortlaufende Daten an, sofern diese keine Ausreißer besitzen (positiv/negativ)\n",
    "    - Ersetzen der Daten durch den Median. Der Median sollte gewählt werden wenn die Daten Ausreißer besitzen.\n",
    "\n",
    "\n",
    "- Behandlung von kategorischen Werten\n",
    "    - Ersetzen der Daten durch die am meisten vorkommende Kategorie\n",
    "    - Ersetzen der Daten durch eine eigene Kategorie\n",
    "    - Ersetzen der Daten durch vorhersagende Modelle, welche versuchen die fehlenden Daten zu schätzen\n",
    "    \n",
    "\n",
    "Das andere Problem sind Fehlerhafte Daten. Oft ist nicht bekannt ob die vorliegenden Daten fehlerhaft sind, was das korrekte Verständnis der erhobenen Daten voraussetzt. Quellen solcher fehlerhafter Daten können von falscher Erhebung über Fremdquellen (Maschine schreibt falsche Werte), bis zu Übertragungsfehlern reichen. Oft kann man nichts gegen diese Fehler unternehmen. Wie auch fehlende Daten, können fehlerhafte Daten ebenfalls verarbeitet werden. Die oben genannten Ansätze sind im gleichen Maße anwendbar.\n",
    "\n",
    "Deshalb müssen Daten, bevor ein Maschinelles Lernen-Modell erstellt wird, verstanden und bereinigt werden. Dieser Teil nimmt oft am meisten Zeit in Anspruch.\n",
    "\n",
    "Interessant ist jedoch, wie unser eigener Datensatz aussieht. Dazu wollen wir diesen näher betrachten:\n",
    "\n",
    "\n",
    "Die Dokumentation des vorliegenden Datensatzes zeigt sehr schön welche Daten vorhanden sind, woher deren Ursprung ist und wie der Datensatz erhoben wurde. Jedoch sind dort auch Unstimmigkeiten im Datensatz, welche sich die erhebenden nicht einmal erklären können. Trotz allem müssen die Fehlerhaften Daten entfernt oder bereinigt werden. Sollten diese Fehlerhaften Datensätze nicht weiter verarbeitet werden, so kann es sein, dass die Algorithmen Fehlerhaftes und falsches Verhalten lernen und dieses auf Produktionsdaten anwenden. Bedeutet, der Algorithmus gibt Falsche Lösungen vor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = widgets.Output()\n",
    "def show_grid(b):\n",
    "    with output:\n",
    "        grid = qgrid.show_grid(masterprojekt_daten, show_toolbar=False, grid_options={'forceFitColumns': False, 'editable': False})\n",
    "        display(grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "button_layout = widgets.Layout(width='auto', height='auto')\n",
    "button = widgets.Button(description='Daten anzeigen', layout=button_layout)\n",
    "print('Messdaten:')\n",
    "display(button, output)\n",
    "button.on_click(show_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "@interact\n",
    "def plotAllColumnsPlotly(column=masterprojekt_daten.columns): \n",
    "    hist = go.Histogram(x=masterprojekt_daten[column].values, nbinsx=50, showlegend=False)\n",
    "    scatter = go.Scatter(x=masterprojekt_daten[column].values, marker=dict(size=0.05), showlegend=False)\n",
    "    line = go.Scatter(x=masterprojekt_daten[column].values, y=masterprojekt_daten[column].values, mode='lines', showlegend=False)\n",
    "    dist_data = [masterprojekt_daten[column].values]\n",
    "    dist = ff.create_distplot(dist_data, group_labels=[column])\n",
    "    density = go.Figure(data=dist, layout=dict(title=str('Distribution ' + column)))\n",
    "    \n",
    "        \n",
    "    # specs=[[{}, {}], [{'colspan': 2}, None]],\n",
    "    fig = make_subplots(rows=2, cols=2, specs=[[{}, {}], [{'colspan': 2}, None]],\\\n",
    "                              subplot_titles=(str('Histogram ' + column), str('Line ' + column), str('Scatter ' + column)))\n",
    "    \n",
    "    fig.append_trace(hist, 1,1)\n",
    "    fig.append_trace(line, 1,2)\n",
    "    fig.append_trace(scatter, 2,1)\n",
    "    #fig.append_trace(fig2.show(), 2,2)\n",
    "\n",
    "    fig['layout'].update(height=800, width=950)\n",
    "    iplot(fig)\n",
    "    iplot(density)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.figure_factory as ff\n",
    "\n",
    "selection = widgets.SelectMultiple(\n",
    "    options=masterprojekt_daten.columns,\n",
    "    rows=5,\n",
    "    value=['Energy_Savings.Active_Power_Spindle'],\n",
    "    description='Spalten',\n",
    "    disabled=False,\n",
    "    layout=Layout(width=\"40%\")\n",
    ")\n",
    "\n",
    "@interact(selection=selection)\n",
    "def confusionMatrix(selection):\n",
    "    df_correlation=masterprojekt_daten.corr()\n",
    "    df_correlation.dropna(thresh=1,inplace=True)\n",
    "    # df_correlation = df_correlation.drop(columns=['Label'])\n",
    "    plt.figure(figsize=(20,20))\n",
    "    fig = go.Figure(data=go.Heatmap(z=df_correlation, x=selection, y=selection))\n",
    "\n",
    "    iplot(fig)\n",
    "# poff.plot(fig, './correlation.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selection = widgets.SelectMultiple(\n",
    "    options=masterprojekt_daten.columns,\n",
    "    rows=5,\n",
    "    value=['Energy_Savings.Active_Power_Spindle'],\n",
    "    description='Spalten',\n",
    "    disabled=False,\n",
    "    layout=Layout(width=\"40%\")\n",
    ")\n",
    "\n",
    "\n",
    "#@widgets.interact_manual(selection=selection)\n",
    "@interact(selection=selection)\n",
    "def pairPlot(selection):\n",
    "    ax = sns.pairplot(masterprojekt_daten, hue='Label', vars=[x for x in selection], \n",
    "                palette='husl', height=3)\n",
    "    ax.map(sns.regplot)\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
